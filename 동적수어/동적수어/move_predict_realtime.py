import cv2
import numpy as np
import mediapipe as mp
import joblib
import os
from tensorflow.keras.models import load_model
from collections import deque
from PIL import ImageFont, ImageDraw, Image  # ì¶”ê°€

# í•œê¸€ í°íŠ¸ ê²½ë¡œ ì„¤ì • (ìœˆë„ìš° ê¸°ì¤€)
font_path = "C:/Windows/Fonts/malgun.ttf"
font = ImageFont.truetype(font_path, 32)  # í¬ê¸° 32 ì •ë„ê°€ ì ë‹¹

# ğŸ‘‰ OpenCV í”„ë ˆì„ì— í•œê¸€ì„ ê·¸ë ¤ì£¼ëŠ” í•¨ìˆ˜
def draw_korean_text(cv_img, text, position=(30, 50), color=(0, 255, 0)):
    # OpenCV â†’ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)
    pil_img = Image.fromarray(cv_img)

    draw = ImageDraw.Draw(pil_img)
    draw.text(position, text, font=font, fill=color)

    # ë‹¤ì‹œ OpenCV í¬ë§·ìœ¼ë¡œ ë³€í™˜
    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)


# í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ë‹¨ê³„ ìƒìœ„ í´ë”ë¡œ ì´ë™
CURRENT_DIR = os.path.dirname(__file__)                   # hand_tracking/ë™ì ìˆ˜ì–´/ë™ì ìˆ˜ì–´
BASE_DIR = os.path.abspath(os.path.join(CURRENT_DIR, ".."))  # hand_tracking/ë™ì ìˆ˜ì–´

# ëª¨ë¸ê³¼ ì¸ì½”ë” ê²½ë¡œ ì„¤ì •
model_path = os.path.join(BASE_DIR, "dynamic_gesture_model_lstm.h5")
encoder_path = os.path.join(BASE_DIR, "label_encoder_lstm.pkl")

# ê²½ë¡œ í™•ì¸ ë¡œê·¸
print(f"[INFO] ëª¨ë¸ ê²½ë¡œ: {model_path}")
print(f"[INFO] ì¸ì½”ë” ê²½ë¡œ: {encoder_path}")

# ë¡œë”© ì‹œë„
model = load_model(model_path)
label_encoder = joblib.load(encoder_path)

# ğŸ¯ ì„¤ì •
sequence_length = 30
confidence_threshold = 0.8
frame_queue = deque(maxlen=sequence_length)

# MediaPipe ì„¤ì •
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

cap = cv2.VideoCapture(0)
print("[INFO] ì‹¤ì‹œê°„ ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ESC í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(image)

    if results.multi_hand_landmarks and len(results.multi_hand_landmarks) == 2:
        row = []
        for hand_landmarks in results.multi_hand_landmarks:
            for lm in hand_landmarks.landmark:
                row.extend([lm.x, lm.y, lm.z])
        frame_queue.append(row)

        if len(frame_queue) == sequence_length:
            input_data = np.array(frame_queue).reshape(1, sequence_length, -1)
            prediction = model.predict(input_data)[0]
            confidence = np.max(prediction)
            predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])[0]

            if confidence >= confidence_threshold:
                frame = draw_korean_text(frame, f"ì˜ˆì¸¡: {predicted_label} ({confidence:.2f})",
                                         position=(30, 50), color=(0, 255, 0))
                print(f"[INFO] ì˜ˆì¸¡: {predicted_label}, í™•ë¥ : {confidence:.2f}")
            else:
                frame = draw_korean_text(frame, "ì˜ˆì¸¡: ì—†ìŒ", position=(30, 50), color=(0, 0, 255))

    else:
        frame = draw_korean_text(frame, "ì†ì´ ë‘ ê°œ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤", position=(30, 50), color=(0, 0, 255))

    cv2.imshow("Dynamic Gesture Recognition", frame)
    if cv2.waitKey(1) & 0xFF == 27:
        break

cap.release()
cv2.destroyAllWindows()